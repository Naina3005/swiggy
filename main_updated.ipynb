{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swiggy Top Rated Restaurants in Delhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this web scraping project where we'll scraped Swiggy's top rated restaurants in Delhi data using python packages like Requests, BeautifulSoup, and Pandas.\n",
    "\n",
    "- In this web scraping project we'll be scraping a list of Top Rated restaurants in Delhi from Swiggy's website. Swiggy is an Indian online food ordering and delivery platform. Here is the link - https://www.swiggy.com/city/delhi/top-rated-collection\n",
    "\n",
    "- We'll grab a list of names, and urls of top rated restaurants in Delhi on Swiggy, and put them in 'rest_page.csv'.\n",
    "\n",
    "- Then we'll grab information like name, cuisine, location, rating, number of ratings, price for two, and url for each restaurant and save them in 'rest_delhi.csv'.\n",
    "\n",
    "- The restaurants information will be stored in a csv file with format:\n",
    "\n",
    "```\n",
    "name,cuisine,location,rating,num_of_rating,price_for_two,url\n",
    "Bhukkad's Kitchen,\"North Indian, Indian\",\"Tis Hazari, Tis Hazari\",5.0,20+ ratings,₹ 199,https://swiggy.com/restaurants/great-indian-khichdi-by-eatfit-chuna-mandi-paharganj-gole-market-delhi-258198\n",
    "SUSHI MACHI,Sushi,\"Hauz Khas, Green Park\",4.9,20+ ratings,₹ 600,https://swiggy.com/restaurants/great-indian-khichdi-by-eatfit-chuna-mandi-paharganj-gole-market-delhi-258198\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Requests to get the page and BeautifulSoup to parse through the page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'}\n",
    "rest_page_url= 'https://www.swiggy.com/city/delhi/top-rated-collection?page=1'\n",
    "\n",
    "response = requests.get(rest_page_url, headers=my_header)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    raise Exception('Failed to load page {}'.format(rest_page_url))\n",
    "\n",
    "page_contents= response.text\n",
    "doc = BeautifulSoup(page_contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the list of restaurant's name, and url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1)Getting Restaurants Name and URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_name_tag= doc.find_all('div', {'class': 'nA6kb'})\n",
    "rest_names=[]\n",
    "for tag in rest_name_tag:\n",
    "    rest_names.append(tag.text)\n",
    "    \n",
    "url_selection_class = \"_1j_Yo\"\n",
    "url_tags= doc.find_all('a', {'class': url_selection_class})\n",
    "rest_urls=[]\n",
    "base_url = 'https://swiggy.com'\n",
    "for tag in url_tags:\n",
    "    rest_urls.append(base_url + tag['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Creating dictionary to put in DataFrame and saving to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_page_dict = {\n",
    "    'name': rest_names,\n",
    "    'url': rest_urls\n",
    "}\n",
    "rest_df = pd.DataFrame(rest_page_dict)\n",
    "rest_df.to_csv('rest_page.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Final code\n",
    "Putting it in for loop to get list of restaurants from all pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "rest_names=[]\n",
    "rest_urls=[]\n",
    "\n",
    "for page in range (1,60):\n",
    "  rest_page_url= 'https://www.swiggy.com/city/delhi/top-rated-collection?page='\n",
    "  my_header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'}\n",
    "  response = requests.get(rest_page_url + str(page), headers=my_header)\n",
    "\n",
    "  #check for response\n",
    "  if response.status_code != 200:\n",
    "    raise Exception('Failed to load page{}'.format(rest_urls))\n",
    "  # parse using beautifulsoup\n",
    "  doc = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    " #getting name\n",
    "  rest_name_tag= doc.find_all('div', {'class': 'nA6kb'})\n",
    "\n",
    "  for tag in rest_name_tag:\n",
    "    rest_names.append(tag.text)\n",
    "  #getting url\n",
    "  url_selection_class = \"_1j_Yo\"\n",
    "  url_tags= doc.find_all('a', {'class': url_selection_class})\n",
    "      \n",
    "  base_url = 'https://swiggy.com'\n",
    "  for tag in url_tags:\n",
    "    rest_urls.append(base_url + tag['href'])\n",
    "#creating dictionary    \n",
    "rest_page_dict = {\n",
    "    'name': rest_names,\n",
    "    'url': rest_urls\n",
    "    }\n",
    "      \n",
    "#puuting DataFrame      \n",
    "rest_df = pd.DataFrame(rest_page_dict)\n",
    "#creating csv file\n",
    "rest_df.to_csv('rest_page.csv', index = None)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting details of each restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Going through each restaurant's url from 'rest_page.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('rest_page.csv', newline='') as csvfile:\n",
    "  reader = csv.DictReader(csvfile)\n",
    "\n",
    "  urls = [row[\"url\"] for row in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2)Using Requests to get the page and BeautifulSoup to parse through the page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_rest = requests.get(urls[0] , headers=my_header)\n",
    "\n",
    "#check for response\n",
    "if response.status_code != 200:\n",
    "    raise Exception('Failed to load page{}'.format(rest_urls))\n",
    " \n",
    "# parse using beautifulsoup\n",
    "rest_doc = BeautifulSoup(response_rest.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) getting details of each url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_cuisine=[]\n",
    "rest_cuisine_tag= rest_doc.find('div', {'class':'_3Plw0 JMACF'})\n",
    "rest_cuisine.append(rest_cuisine_tag.text.strip())\n",
    "\n",
    "rest_location=[]\n",
    "rest_location_tag= rest_doc.find('div', {'class':'Gf2NS _2Y6HW'})\n",
    "rest_location.append(rest_location_tag.text.strip())\n",
    "\n",
    "\n",
    "rest_info_tag= rest_doc.find_all('div', {'class':'_2l3H5'})\n",
    "\n",
    "rest_rating=[]\n",
    "rest_rating.append(rest_info_tag[0].text.strip())\n",
    "\n",
    "price_two=[]\n",
    "price_two.append(rest_info_tag[2].text.strip())\n",
    "\n",
    "num_rating=[]\n",
    "num_rating_tag= rest_doc.find('div', {'class':'_1De48'})\n",
    "num_rating.append(num_rating_tag.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Final code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "with open('rest_page.csv', newline='') as csvfile:\n",
    "  reader = csv.DictReader(csvfile)\n",
    "\n",
    "  urls = [row[\"url\"] for row in reader]\n",
    "  \n",
    "rest_names=[]\n",
    "rest_cuisine=[]\n",
    "rest_location=[]\n",
    "rest_rating=[]\n",
    "price_two=[]\n",
    "num_rating=[]\n",
    "  \n",
    "for url in urls:\n",
    "  response_rest = requests.get(url , headers=my_header)\n",
    "  rest_doc = BeautifulSoup(response_rest.text, 'html.parser')\n",
    "  \n",
    "  rest_name_tag= rest_doc.find('div', {'class': 'OEfxz'})\n",
    "  rest_names.append(rest_name_tag.text)\n",
    "\n",
    "  rest_cuisine_tag= rest_doc.find('div', {'class':'_3Plw0 JMACF'})\n",
    "  rest_cuisine.append(rest_cuisine_tag.text.strip())\n",
    "  rest_location_tag= rest_doc.find('div', {'class':'Gf2NS _2Y6HW'})\n",
    "  rest_location.append(rest_location_tag.text.strip())\n",
    "\n",
    "  rest_info_tag= rest_doc.find_all('div', {'class':'_2l3H5'})\n",
    "\n",
    "  rest_rating.append(rest_info_tag[0].text.strip())\n",
    "  \n",
    "  price_two.append(rest_info_tag[2].text.strip())\n",
    "\n",
    "  num_rating_tag= rest_doc.find('div', {'class':'_1De48'})\n",
    "  num_rating.append(num_rating_tag.text.strip())\n",
    "\n",
    "rest_info_dict={\n",
    "    'name':rest_names,\n",
    "    'cuisine': rest_cuisine,\n",
    "    'location':rest_location,\n",
    "    'rating':rest_rating,\n",
    "    'num_of_rating':num_rating,\n",
    "    'price_for_two':price_two,\n",
    "}\n",
    "\n",
    "rest_info_df =  pd.DataFrame(rest_info_dict)\n",
    "\n",
    "rest_info_df.to_csv('rest_delhi1.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ciao amico!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d23bc0f3468e1047e4a1919b1581627b435af5490757bdf5942c255dc2c18c08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
